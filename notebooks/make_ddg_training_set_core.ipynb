{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext",
    "run_control": {
     "marked": false
    }
   },
   "source": [
    "\n",
    "TODO\n",
    "  - There's some dirty business with the duplication of domains in the `uniprot_domain` table.\n",
    "  - There's also some dirty business with some domains having templates with sequence identity in range ``0 <= x <= 1``, and others having templates with sequence identity in range ``1 <= x <= 100``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "__file__ = 'make_ddg_training_set_core'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-04-06 21:13:14.135309\n"
     ]
    }
   ],
   "source": [
    "%run common_imports.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 - Load data from small-scale studies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "#%% Set parameters\n",
    "subprocess.check_call('mkdir -p ' + constants.protherm_data_path + 'parsed_data{}'.format(version_suffix), shell=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "#%% PART 1 \n",
    "###################################################################################################\n",
    "\n",
    "#%% Create the required database and tables if they don't exist already\n",
    "#    create database elaspic_training;\n",
    "#\n",
    "#    create view elaspic_training.domain as select * from elaspic.domain;\n",
    "#    create view elaspic_training.domain_contact as select * from elaspic.domain_contact;\n",
    "#    create view elaspic_training.provean as select * from elaspic.provean;\n",
    "#\n",
    "#    create table elaspic_training.uniprot_domain like elaspic.uniprot_domain;\n",
    "#    create table elaspic_training.uniprot_domain_template like elaspic.uniprot_domain_template;\n",
    "#    create table elaspic_training.uniprot_domain_model like elaspic.uniprot_domain_model;\n",
    "#    create table elaspic_training.uniprot_domain_mutation like elaspic.uniprot_domain_mutation;\n",
    "#\n",
    "#    create table elaspic_training.uniprot_domain_pair like elaspic.uniprot_domain_pair;\n",
    "#    create table elaspic_training.uniprot_domain_pair_template like elaspic.uniprot_domain_pair_template;\n",
    "#    create table elaspic_training.uniprot_domain_pair_model like elaspic.uniprot_domain_pair_model;\n",
    "#    create table elaspic_training.uniprot_domain_pair_mutation like elaspic.uniprot_domain_pair_mutation;\n",
    "#\n",
    "#    ALTER TABLE `elaspic_training`.`uniprot_domain`\n",
    "#    ADD COLUMN `max_seq_identity` VARCHAR(45) NULL AFTER `path_to_data`;\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "#%% PART 2 - LOAD PROTHERM DATA\n",
    "###################################################################################################\n",
    "\n",
    "protherm_filename_full = constants.local_database_path + 'mutations/ddg/protherm/ProTherm.dat'\n",
    "\n",
    "cwd = os.getcwd()\n",
    "os.chdir(constants.working_path + 'elaspic_tools/mutation_sets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "#%% Create a file that will parse the protherm database dump (WARNING: It may be memory-intensive!)\n",
    "#    %%file _parse_protherm.py\n",
    "#    import os\n",
    "#    import sys\n",
    "#    import cPickle as pickle\n",
    "#    from tempfile import NamedTemporaryFile\n",
    "#    import multiprocessing as mp\n",
    "#\n",
    "#    _ROOT = os.path.abspath(os.path.dirname(__file__))\n",
    "#    sys.path.insert(0, os.path.join(_ROOT, \".\"))\n",
    "#\n",
    "#    from common import constants\n",
    "#    import ddg_parsers\n",
    "#\n",
    "#    n_cores = 8\n",
    "#    path_to_data = '/home/kimlab1/strokach/databases/'\n",
    "#    protherm_filename_full = path_to_data + 'mutations/ddg/protherm/ProTherm.dat'\n",
    "#\n",
    "#    with open(protherm_filename_full) as ifh:\n",
    "#        file_data = ifh.readlines()\n",
    "#\n",
    "#    chunk_size = len(file_data) / float(n_cores)\n",
    "#    chunk_idxs = [0]\n",
    "#    for i in range(n_cores - 1):\n",
    "#        idx = int(chunk_size * (i + 1))\n",
    "#        while not file_data[idx].startswith('//'):\n",
    "#            idx += 1\n",
    "#        chunk_idxs.append(idx+1)\n",
    "#    chunk_idxs.append(len(file_data))\n",
    "#\n",
    "#    def worker(chunk_filename):\n",
    "#        parse_protherm = parsers.ParseProtherm()\n",
    "#        parse_protherm.parse(chunk_filename)\n",
    "#        protherm_data = parse_protherm.data\n",
    "#        print('Almost done {}'.format(i))\n",
    "#        pickle.dump(protherm_data, open(chunk_filename + '.pickle', 'wb'), pickle.HIGHEST_PROTOCOL)\n",
    "#        print('Done {}'.format(i))\n",
    "#\n",
    "#    chunk_filenames = []\n",
    "#    for i in range(n_cores):\n",
    "#        chunk_file = NamedTemporaryFile(delete=False)\n",
    "#        chunk_file.writelines(file_data[chunk_idxs[i]:chunk_idxs[i+1]])\n",
    "#        chunk_file.seek(0)\n",
    "#        chunk_filenames.append(chunk_file.name)\n",
    "#\n",
    "#    jobs = []\n",
    "#    for chunk_filename in chunk_filenames:\n",
    "#        p = mp.Process(target=worker, args=(chunk_filename,))\n",
    "#        p.start()\n",
    "#        jobs.append(p)\n",
    "#\n",
    "#    for j in jobs:\n",
    "#        j.join()\n",
    "#\n",
    "#    protherm_data = []\n",
    "#    for chunk_filename in chunk_filenames:\n",
    "#        protherm_data_chunk = pickle.load(open(chunk_filename + '.pickle'))\n",
    "#        protherm_data.extend(protherm_data_chunk)\n",
    "#        os.remove(chunk_filename)\n",
    "#        os.remove(chunk_filename + '.pickle')\n",
    "#\n",
    "#    pickle.dump(protherm_data, open(protherm_filename_full + '.parsed.pickle', 'wb'), pickle.HIGHEST_PROTOCOL)\n",
    "#\n",
    "\n",
    "\n",
    "##%% Run the file created above, and delete it if everything finishes successfully\n",
    "#    %run _parse_protherm.py\n",
    "#    rm _parse_protherm.py\n",
    "\n",
    "\n",
    "##%% Load results\n",
    "#    protherm_dict = pickle.load(open(protherm_filename_full + '.parsed.pickle'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "#%% Parse ProTherm.dat in parallel\n",
    "\n",
    "#%% Stuff for parallel processing\n",
    "from IPython.parallel import Client\n",
    "rc = Client()\n",
    "dview = rc[:]\n",
    "print('Connected to {} workers'.format(len(rc)))\n",
    "\n",
    "async_result = dview.execute('%load_ext autoreload', silent=False, block=True)\n",
    "print(''.join(async_result.stdout))\n",
    "async_result = dview.execute('%autoreload 2', silent=False, block=True)\n",
    "print(''.join(async_result.stdout))\n",
    "\n",
    "\n",
    "# Divide one big file into many small ones, taking care to create breaks at appropriate positions\n",
    "def divide_protherm_file(protherm_filename_full, num_chunks):\n",
    "    with open(protherm_filename_full) as ifh:\n",
    "        file_data = ifh.readlines()\n",
    "\n",
    "    chunk_size = len(file_data) / float(num_chunks)\n",
    "    chunk_idxs = [0]\n",
    "    for i in range(num_chunks - 1):\n",
    "        idx = int(chunk_size * (i + 1))\n",
    "        while not file_data[idx].startswith('//'):\n",
    "            idx += 1\n",
    "        chunk_idxs.append(idx+1)\n",
    "    chunk_idxs.append(len(file_data))\n",
    "\n",
    "    chunk_filenames = []\n",
    "    for i in range(num_chunks):\n",
    "        chunk_file = NamedTemporaryFile(delete=False)\n",
    "        chunk_file.writelines(file_data[chunk_idxs[i]:chunk_idxs[i+1]])\n",
    "        chunk_file.seek(0)\n",
    "        chunk_filenames.append(chunk_file.name)\n",
    "    return chunk_filenames\n",
    "\n",
    "# Define worker for parallel processing\n",
    "def worker(chunk_filename):\n",
    "    parse_protherm = parsers.ParseProtherm()\n",
    "    parse_protherm.parse(chunk_filename)\n",
    "    protherm_data = parse_protherm.data\n",
    "    return protherm_data\n",
    "\n",
    "\n",
    "# Magic\n",
    "async_result = dview.execute('from elaspic_tools.mutation_sets import parsers', silent=False, block=True)\n",
    "print(''.join(async_result.stdout))\n",
    "chunk_filenames = divide_protherm_file(protherm_filename_full, len(rc))\n",
    "\n",
    "protherm_dict = [x for xx in dview.map_sync(worker, chunk_filenames) for x in xx]\n",
    "pickle.dump(protherm_dict, open(protherm_filename_full + '.parsed.pickle', 'wb'), pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "#%%% Load parsed Protherm data\n",
    "protherm_dict = pickle.load(open(protherm_filename_full + '.parsed.pickle'))\n",
    "protherm_df = pd.DataFrame(protherm_dict)\n",
    "protherm_df = protherm_df.rename(columns={'mutation_uniprot': 'uniprot_mutation'})\n",
    "protherm_df['uniprot_mutation'] = (\n",
    "    protherm_df[['pdb_aa', 'uniprot_mutation']]\n",
    "    .apply(lambda x: (x[0][0] + x[1][1:]) if (pd.notnull(x[0]) and pd.notnull(x[1])) else np.nan, axis=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "#%% Manually correct some errors\n",
    "# Acylphosphatase\n",
    "protherm_df.loc[\n",
    "    (pd.isnull(protherm_df['uniprot_name'])) &\n",
    "    (protherm_df['protein_name'] == 'Acylphosphatase'),\n",
    "    'uniprot_name'] = 'ACYP1_HUMAN'\n",
    "protherm_df.loc[\n",
    "    (pd.isnull(protherm_df['uniprot_id'])) &\n",
    "    (protherm_df['protein_name'] == 'Acylphosphatase'),\n",
    "    'uniprot_id'] = 'P07311'\n",
    "mutations = protherm_df[\n",
    "    (pd.notnull(protherm_df['mutation'])) &\n",
    "    (protherm_df['protein_name'] == 'Acylphosphatase')]\n",
    "protherm_df.loc[\n",
    "    (pd.notnull(protherm_df['mutation'])) &\n",
    "    (protherm_df['protein_name'] == 'Acylphosphatase'),\n",
    "    'uniprot_mutation'] = mutations\n",
    "\n",
    "\n",
    "# Alkaline phosphatase\n",
    "protherm_df.loc[\n",
    "    (pd.isnull(protherm_df['uniprot_name'])) &\n",
    "    (protherm_df['protein_name'] == 'Alkaline phosphatase'),\n",
    "    'uniprot_name'] = 'PPB_YEAST'\n",
    "protherm_df.loc[\n",
    "    (pd.isnull(protherm_df['uniprot_id'])) &\n",
    "    (protherm_df['protein_name'] == 'Alkaline phosphatase'),\n",
    "    'uniprot_id'] = 'P11491'\n",
    "\n",
    "\n",
    "# Arginine kinase\n",
    "protherm_df.loc[\n",
    "    (pd.isnull(protherm_df['uniprot_name'])) &\n",
    "    (protherm_df['protein_name'] == 'Arginine kinase'),\n",
    "    'uniprot_name'] = 'KARG_DROME'\n",
    "protherm_df.loc[\n",
    "    (pd.isnull(protherm_df['uniprot_id'])) &\n",
    "    (protherm_df['protein_name'] == 'Arginine kinase'),\n",
    "    'uniprot_id'] = 'P48610'\n",
    "\n",
    "\n",
    "# Eglin C\n",
    "protherm_df.loc[\n",
    "    (pd.isnull(protherm_df['uniprot_name'])) &\n",
    "    (protherm_df['protein_name'] == 'Eglin C'),\n",
    "    'uniprot_name'] = 'ICIC_HIRME'\n",
    "protherm_df.loc[\n",
    "    (pd.isnull(protherm_df['uniprot_id'])) &\n",
    "    (protherm_df['protein_name'] == 'Eglin C'),\n",
    "    'uniprot_id'] = 'P01051'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "#%% Format error fields\n",
    "\n",
    "# Select ddG_H2O if it is availible, else select ddG\n",
    "protherm_df['ddg_best'] = [x[0] if pd.notnull(x[0]) else x[1] for x in protherm_df[['ddG_H2O', 'ddG']].values]\n",
    "\n",
    "# Reverse the sign because protherm defines ddG and dTm as (mutant) - (wildtype)\n",
    "# http://www.abren.net/protherm/protherm_knownproblems.php\n",
    "change_columns = ['dTm', 'ddG', 'ddG_H2O', 'ddg_best']\n",
    "for column in change_columns:\n",
    "    protherm_df[column] = -protherm_df[column]\n",
    "\n",
    "\n",
    "def format_errors(error_field):\n",
    "    if not error_field:\n",
    "        return ''\n",
    "    error_messages = []\n",
    "    error_subfields = [e.strip().strip(';') for e in error_field.split(':')]\n",
    "    for i in range(len(error_subfields)-1):\n",
    "        if error_subfields[i].endswith('Error'):\n",
    "            error_message = error_subfields[i+1].rstrip('Line').strip(';').strip()\n",
    "            if re.search('Cannot convert entry .* to float', error_message):\n",
    "                error_message = 'Cannot convert entry to float'\n",
    "            if error_message not in error_messages:\n",
    "                error_messages.append(error_message)\n",
    "    return '; ' + '; '.join(error_messages)\n",
    "\n",
    "\n",
    "def classify_erros(error_messages):\n",
    "    if not error_messages:\n",
    "        return [0, 'Mapped successfully']\n",
    "    if 'Wild-type protein' in error_messages:\n",
    "        return [1, 'Wild-type']\n",
    "    if 'No ddG and dTm score provided' in error_messages:\n",
    "        return [1, 'No ddG and dTm scores']\n",
    "    if ('Keeping only single mutation variants' in error_messages or\n",
    "        'Only considering single amino acid substitutions' in error_messages):\n",
    "            return [2, 'Multiple mutations']\n",
    "    return [10, 'Mapping error']\n",
    "\n",
    "\n",
    "def get_idx_where_better_exists(protherm_df):\n",
    "    \"\"\"Remove cases where the same uniprot_id-uniprot_mutation pair exists in a row with and without remarks\n",
    "    \"\"\"\n",
    "    uniprot_mutation_no_remarks_ddg = set(\n",
    "        protherm_df[\n",
    "            pd.isnull(protherm_df['remarks']) &\n",
    "            pd.notnull(protherm_df['ddg_best'])\n",
    "        ][['uniprot_id', 'uniprot_mutation']].apply(tuple, axis=1)\n",
    "    )\n",
    "    uniprot_mutation_no_remarks_dtm = set(\n",
    "        protherm_df[\n",
    "            pd.isnull(protherm_df['remarks']) &\n",
    "            pd.notnull(protherm_df['dTm'])\n",
    "        ][['uniprot_id', 'uniprot_mutation']].apply(tuple, axis=1)\n",
    "    )\n",
    "\n",
    "    index_bad_ddg = protherm_df[\n",
    "        (pd.notnull(protherm_df['remarks'])) &\n",
    "        (pd.notnull(protherm_df['ddg_best'])) &\n",
    "        (protherm_df[['uniprot_id', 'uniprot_mutation']]\n",
    "            .apply(tuple, axis=1)\n",
    "            .isin(uniprot_mutation_no_remarks_ddg))\n",
    "    ].index\n",
    "\n",
    "    index_bad_dtm = protherm_df[\n",
    "        (pd.notnull(protherm_df['remarks'])) &\n",
    "        (pd.isnull(protherm_df['ddg_best'])) &\n",
    "        (pd.notnull(protherm_df['dTm'])) &\n",
    "        (protherm_df[['uniprot_id', 'uniprot_mutation']]\n",
    "            .apply(tuple, axis=1)\n",
    "            .isin(uniprot_mutation_no_remarks_dtm))\n",
    "    ].index\n",
    "\n",
    "    return list(set(index_bad_ddg) | set(index_bad_dtm))\n",
    "\n",
    "\n",
    "protherm_df['error_messages'] = protherm_df['errors'].apply(format_errors)\n",
    "\n",
    "protherm_df['error_messages'] = (\n",
    "    protherm_df['error_messages']\n",
    "    .where(protherm_df['mutation'] != 'wild',\n",
    "           protherm_df['error_messages'] + '; Wild-type protein')\n",
    ")\n",
    "protherm_df['error_messages'] = (\n",
    "    protherm_df['error_messages']\n",
    "    .where(pd.notnull(protherm_df['ddg_best']) | pd.notnull(protherm_df['dTm']),\n",
    "           protherm_df['error_messages'] + '; No ddG and dTm score provided')\n",
    ")\n",
    "protherm_df['error_messages'] = (\n",
    "    protherm_df['error_messages']\n",
    "    .where(pd.notnull(protherm_df['uniprot_id']),\n",
    "           protherm_df['error_messages'] + '; Uniprot id is missing')\n",
    ")\n",
    "protherm_df['error_messages'] = (\n",
    "    protherm_df['error_messages']\n",
    "    .where(pd.notnull(protherm_df['uniprot_mutation']),\n",
    "           protherm_df['error_messages'] + '; Uniprot mutation is missing')\n",
    ")\n",
    "\n",
    "idxs_to_drop = get_idx_where_better_exists(protherm_df)\n",
    "protherm_df.loc[idxs_to_drop, 'error_messages'] = (\n",
    "    protherm_df.loc[idxs_to_drop, 'error_messages'] + '; A better version of this mutation exists'\n",
    ")\n",
    "\n",
    "# Check some of the `idxs_to_drop` mutations\n",
    "#protherm_df.loc[16389, ['uniprot_id', 'uniprot_mutation', 'ddg_best', 'dTm', 'remarks', 'error_messages']]\n",
    "#protherm_df[\n",
    "#    (protherm_df['uniprot_id'] == 'P00720') &\n",
    "#    (protherm_df['uniprot_mutation'] == 'V111A')\n",
    "#][['uniprot_id', 'uniprot_mutation', 'ddg_best', 'dTm', 'remarks', 'error_messages']]\n",
    "\n",
    "protherm_df['error_code'], protherm_df['error_category'] = zip(*protherm_df['error_messages'].apply(classify_erros))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "#%% The same protein-mutation pair may occur multiple times\n",
    "protherm_df_good = (\n",
    "    protherm_df[\n",
    "        (protherm_df['mutation'] != 'wild') &\n",
    "        (protherm_df['error_messages'] == '')\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Average the thermidynamic parameters over all occurances of the same protein-mutation\n",
    "data_columns = ['Tm', 'dG', 'dG_H2O', 'dHvH', 'dTm', 'ddG', 'ddG_H2O', 'ddg_best']\n",
    "protherm_df_good_gp = protherm_df_good.groupby(['uniprot_id', 'uniprot_mutation'])\n",
    "protherm_df_good_unique = (\n",
    "    protherm_df_good_gp\n",
    "    .agg(tuple)\n",
    "    .merge(\n",
    "        protherm_df_good_gp[data_columns]\n",
    "        .agg(np.nanmean)\n",
    "        .rename(columns=lambda c: c if c not in data_columns else c + '_mean'),\n",
    "        left_index=True, right_index=True)\n",
    "    .merge(\n",
    "        protherm_df_good_gp[data_columns]\n",
    "        .agg(np.nanstd)\n",
    "        .rename(columns=lambda c: c if c not in data_columns else c + '_std'),\n",
    "        left_index=True, right_index=True)\n",
    "    .merge(\n",
    "        protherm_df_good_gp[data_columns]\n",
    "        .agg(np.nanmedian)\n",
    "        .rename(columns=lambda c: c if c not in data_columns else c + '_median'),\n",
    "        left_index=True, right_index=True)\n",
    "    .merge(\n",
    "        protherm_df_good_gp['mutation']\n",
    "        .agg({'count': len}),\n",
    "        left_index=True, right_index=True)\n",
    "    .merge(\n",
    "        protherm_df_good_gp\n",
    "        .agg({'remarks': lambda x: tuple(x)}),\n",
    "        left_index=True, right_index=True)\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Use ddG_H2O if it is availible, else ddG\n",
    "def get_first_not_null(row):\n",
    "    for value in row:\n",
    "        if pd.notnull(value):\n",
    "            return value\n",
    "    return np.nan\n",
    "\n",
    "protherm_df_good_unique['ddg_all_mean'] = (\n",
    "    protherm_df_good_unique[['ddG_H2O_mean', 'ddG_mean']].apply(get_first_not_null, axis=1)\n",
    ")\n",
    "protherm_df_good_unique['ddg_all_median'] = (\n",
    "    protherm_df_good_unique[['ddG_H2O_median', 'ddG_median']].apply(get_first_not_null, axis=1)\n",
    ")\n",
    "protherm_df_good_unique['ddg_all_std'] = (\n",
    "    protherm_df_good_unique[['ddG_H2O_std', 'ddG_std']].apply(get_first_not_null, axis=1)\n",
    ")\n",
    "\n",
    "assert sum(protherm_df_good_unique.duplicated(subset=['uniprot_id', 'uniprot_mutation'])) == 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "#%% Add sequence information to each mutated protein\n",
    "engine = sa.create_engine('mysql://elaspic:elaspic@192.168.6.19/uniprot_kb')\n",
    "sql_query = \"\"\"\n",
    "select *\n",
    "from uniprot_kb.uniprot_sequence\n",
    "where uniprot_id in ('{}') ;\n",
    "\"\"\".format(\"', '\".join(protherm_df_good_unique['uniprot_id'].drop_duplicates()))\n",
    "uniprot_sequences = pd.read_sql_query(sql_query, engine)\n",
    "\n",
    "protherm_df_good_unique_wseq = protherm_df_good_unique.merge(uniprot_sequences, on=['uniprot_id'])\n",
    "protherm_df_good_unique_wseq['sequence_match'] = [\n",
    "    parsers.mutation_in_sequence(*x) for x\n",
    "    in protherm_df_good_unique_wseq[['uniprot_mutation', 'uniprot_sequence']].values]\n",
    "assert all(protherm_df_good_unique_wseq['sequence_match'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "#%%\n",
    "protherm_df.to_pickle(\n",
    "    constants.protherm_data_path + 'parsed_data{}/protherm_df.pickle'.format(version_suffix))\n",
    "protherm_df_good.to_pickle(\n",
    "    constants.protherm_data_path + 'parsed_data{}/protherm_df_good.pickle'.format(version_suffix))\n",
    "protherm_df_good_unique.to_pickle(\n",
    "    constants.protherm_data_path + 'parsed_data{}/protherm_df_good_unique.pickle'.format(version_suffix))\n",
    "protherm_df_good_unique_wseq.to_pickle(\n",
    "    constants.protherm_data_path + 'parsed_data{}/protherm_df_good_unique_wseq.pickle'.format(version_suffix))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "#%% PART 3 - JOIN PROTHERM AND SMALL-SCALE STUDIES\n",
    "###################################################################################################\n",
    "\n",
    "#%%\n",
    "abdellah_et_al_up_mut = pd.read_pickle(\n",
    "    constants.protherm_data_path + 'parsed_data{}/small_studies_df.pickle'.format(version_suffix)\n",
    ")\n",
    "protherm_df_good_unique_wseq = pd.read_pickle(\n",
    "    constants.protherm_data_path + 'parsed_data{}/protherm_df_good_unique_wseq.pickle'.format(version_suffix)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "#%% Concatenate protherm data with data from small-scale studies\n",
    "core_mut_wseq = pd.concat([protherm_df_good_unique_wseq, abdellah_et_al_up_mut], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "#%% Add domain information to each mutated protein\n",
    "engine = sa.create_engine('mysql://elaspic:elaspic@192.168.6.19/elaspic')\n",
    "sql_query = \"\"\"\n",
    "select *\n",
    "from elaspic.uniprot_domain ud\n",
    "join elaspic.uniprot_domain_template udt using (uniprot_domain_id)\n",
    "where uniprot_id in ('{}') ;\n",
    "\"\"\".format(\"', '\".join(protherm_df_good_unique_wseq['uniprot_id'].drop_duplicates()))\n",
    "elaspic_domains = pd.read_sql_query(sql_query, engine)\n",
    "\n",
    "core_mut_wseq_wdom = core_mut_wseq.merge(elaspic_domains, left_on=['uniprot_id'], right_on=['uniprot_id'])\n",
    "core_mut_wseq_wdom['mutation_inside_domain'] = (\n",
    "    core_mut_wseq_wdom[['uniprot_mutation', 'domain_def']]\n",
    "    .apply(parsers.mutation_inside_domain, axis=1, raw=True)\n",
    ")\n",
    "core_mut_wseq_wdom = core_mut_wseq_wdom[core_mut_wseq_wdom['mutation_inside_domain']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "#%% Duplicate uniprot_domain columns for different sequence identity thresholds\n",
    "\n",
    "### IMPORTANT!!! ORGANISM NAME MUST BE SET TO 'training' IN ORDER TO KEEP THE REAL AND TRAINING DATA SEGREGATED\n",
    "\n",
    "# TODO: This should be done after fining templates!!!\n",
    "\n",
    "### Mutation from wild-type to mutant\n",
    "uniprot_domain_wt = core_mut_wseq_wdom.drop_duplicates(subset=['uniprot_domain_id', 'uniprot_id', 'uniprot_mutation']).copy()\n",
    "uniprot_domain_wt['db'] = 'elaspic'\n",
    "uniprot_domain_wt['uniprot_id'] = uniprot_domain_wt['uniprot_id'] + '_wt'\n",
    "uniprot_domain_wt['path_to_data'] = None\n",
    "uniprot_domain_wt['organism_name'] = 'training'\n",
    "uniprot_domain_wt['uniprot_name'] = uniprot_domain_wt['uniprot_id'].apply(lambda x: x.split('_')[0]) + '_training'\n",
    "\n",
    "\n",
    "### Mutation from mutant to wild-type\n",
    "uniprot_domain_mut = core_mut_wseq_wdom.drop_duplicates(subset=['uniprot_domain_id', 'uniprot_id', 'uniprot_mutation']).copy()\n",
    "uniprot_domain_mut['db'] = 'elaspic'\n",
    "uniprot_domain_mut['uniprot_id'] = uniprot_domain_mut['uniprot_id'] + '_' + uniprot_domain_mut['uniprot_mutation']\n",
    "uniprot_domain_mut['path_to_data'] = None\n",
    "uniprot_domain_mut['organism_name'] = 'training'\n",
    "uniprot_domain_mut['uniprot_name'] = uniprot_domain_mut['uniprot_id'].apply(lambda x: x.split('_')[0]) + '_training'\n",
    "\n",
    "# Introduce the mutation into the uniprot sequence\n",
    "uniprot_domain_mut['uniprot_sequence'] = (\n",
    "    uniprot_domain_mut[['uniprot_sequence', 'uniprot_mutation']]\n",
    "    .apply(parsers.mutate_sequence, axis=1, raw=True)\n",
    ")\n",
    "uniprot_domain_mut['uniprot_mutation'] = uniprot_domain_mut['uniprot_mutation'].apply(lambda x: x[-1] + x[1:-1] + x[0])\n",
    "for column in ['dTm', 'ddG', 'ddG_H2O', 'ddg_best', 'ddg_all']:\n",
    "    uniprot_domain_mut[column + '_mean'] = uniprot_domain_mut[column + '_mean'].apply(lambda x: -x)\n",
    "    uniprot_domain_mut[column + '_median'] = uniprot_domain_mut[column + '_median'].apply(lambda x: -x)\n",
    "\n",
    "print(len(uniprot_domain_wt))\n",
    "print(len(uniprot_domain_mut))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "#%% Add wild-type and mutant sequences to the MySQL database\n",
    "engine = sa.create_engine('mysql://elaspic:elaspic@192.168.6.19/uniprot_kb')\n",
    "sql_query = \"\"\"\n",
    "select uniprot_id\n",
    "from uniprot_kb.uniprot_sequence\n",
    "where db = 'elaspic' ;\n",
    "\"\"\"\n",
    "uniprots_uploaded_previously = pd.read_sql_query(sql_query, engine)\n",
    "set_of_uniprots_uploaded_previously = set(uniprots_uploaded_previously['uniprot_id'].values)\n",
    "\n",
    "uniprot_sequence_columns = [\n",
    "    'db', 'uniprot_id', 'uniprot_name', 'protein_name', 'organism_name',\n",
    "    'gene_name', 'protein_existence', 'sequence_version', 'uniprot_sequence']\n",
    "uniprot_domain_wt[\n",
    "        ~(uniprot_domain_wt['uniprot_id'].isin(set_of_uniprots_uploaded_previously))\n",
    "    ][uniprot_sequence_columns].to_sql('uniprot_sequence', engine, if_exists='append', index=False)\n",
    "uniprot_domain_mut[\n",
    "        ~(uniprot_domain_mut['uniprot_id'].isin(set_of_uniprots_uploaded_previously))\n",
    "    ][uniprot_sequence_columns].to_sql('uniprot_sequence', engine, if_exists='append', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "#%% Save results\n",
    "core_mut_wseq.to_pickle(constants.protherm_data_path + 'parsed_data{}/core_mut_wseq.pickle'.format(version_suffix))\n",
    "core_mut_wseq_wdom.to_pickle(constants.protherm_data_path + 'parsed_data{}/core_mut_wseq_wdom.pickle'.format(version_suffix))\n",
    "uniprot_domain_wt.to_pickle(constants.protherm_data_path + 'parsed_data{}/uniprot_domain_wt.pickle'.format(version_suffix))\n",
    "uniprot_domain_mut.to_pickle(constants.protherm_data_path + 'parsed_data{}/uniprot_domain_mut.pickle'.format(version_suffix))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "#%% PART 4 - PERFORM ALIGNMENTS TO FIND TEMPLATES AT DIFFERENT SEQUENCE IDENTITY THRESHOLDS\n",
    "###################################################################################################\n",
    "\n",
    "#%%\n",
    "uniprot_domain_wt = pd.read_pickle(constants.protherm_data_path + 'parsed_data{}/uniprot_domain_wt.pickle'.format(version_suffix))\n",
    "uniprot_domain_mut = pd.read_pickle(constants.protherm_data_path + 'parsed_data{}/uniprot_domain_mut.pickle'.format(version_suffix))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "#%% Stuff for parallel processing\n",
    "from IPython.parallel import Client\n",
    "rc = Client()\n",
    "dview = rc[:]\n",
    "print('Connected to {} workers'.format(len(rc)))\n",
    "\n",
    "async_result = dview.execute('%load_ext autoreload', silent=False, block=True)\n",
    "print(''.join(async_result.stdout))\n",
    "async_result = dview.execute('%autoreload 2', silent=False, block=True)\n",
    "print(''.join(async_result.stdout))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "#%%\n",
    "uniprot_domain_wt['unique_id'] = range(len(uniprot_domain_wt))\n",
    "uniprot_domain_mut['unique_id'] = range(len(uniprot_domain_wt), len(uniprot_domain_wt) + len(uniprot_domain_mut))\n",
    "\n",
    "key_columns = ['unique_id', 'uniprot_id', 'uniprot_mutation', 'domain_def', 'uniprot_sequence']\n",
    "\n",
    "\n",
    "def worker(df):\n",
    "#    blast_results_mutdom_df = df.apply(parsers.get_templates, axis=1, raw=True)\n",
    "#    templates_df = blast_results_mutdom_df.apply(parsers.stratify_results_by_identity, axis=1, raw=True)\n",
    "    blast_results_mutdom_list = [\n",
    "        parsers.get_templates(x) for x in df.values]\n",
    "\n",
    "    templates_list = []\n",
    "    failed_list = []\n",
    "    for blast_results_mutdom in blast_results_mutdom_list:\n",
    "        try:\n",
    "            template = parsers.stratify_results_by_identity(blast_results_mutdom)\n",
    "            templates_list.append(template)\n",
    "        except:\n",
    "            failed_list.append(blast_results_mutdom)\n",
    "    return templates_list, failed_list\n",
    "\n",
    "def get_df_chunks(df):\n",
    "    return [x[1] for x in df.groupby(np.arange(len(df)) / (len(df) / len(rc) + 1))]\n",
    "\n",
    "templates_and_failed_wt = dview.map_sync(worker, get_df_chunks(uniprot_domain_wt[key_columns]))\n",
    "templates_and_failed_mut = dview.map_sync(worker, get_df_chunks(uniprot_domain_mut[key_columns]))\n",
    "\n",
    "# TODO: Figure out why so many failed\n",
    "templates_wt_df = pd.concat([x for xx in templates_and_failed_wt for x in xx[0]], ignore_index=True)\n",
    "templates_mut_df = pd.concat([x for xx in templates_and_failed_mut for x in xx[0]], ignore_index=True)\n",
    "\n",
    "\n",
    "del uniprot_domain_wt['ΔTagg']\n",
    "del uniprot_domain_mut['ΔTagg']\n",
    "\n",
    "uniprot_domain_wt_wtemplates = uniprot_domain_wt.merge(templates_wt_df, on=['unique_id'], suffixes=('_old', ''))\n",
    "uniprot_domain_mut_wtemplates = uniprot_domain_mut.merge(templates_mut_df, on=['unique_id'], suffixes=('_old', ''))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "#%%\n",
    "uniprot_domain_wt_wtemplates['alignment_identity'] = uniprot_domain_wt_wtemplates['alignment_identity'] * 100\n",
    "uniprot_domain_wt_wtemplates['alignment_coverage'] = uniprot_domain_wt_wtemplates['alignment_coverage'] * 100\n",
    "uniprot_domain_mut_wtemplates['alignment_identity'] = uniprot_domain_mut_wtemplates['alignment_identity'] * 100\n",
    "uniprot_domain_mut_wtemplates['alignment_coverage'] = uniprot_domain_mut_wtemplates['alignment_coverage'] * 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "#%% Correct sequence identity values\n",
    "\n",
    "# Blast sometimes gives slightly different sequence coverage and sequence identity values,\n",
    "# which screws up downstream analysis which depends on unique rows\n",
    "\n",
    "# Here we assume that each ``unique_id_columns`` tuple should have a unque\n",
    "# sequence identity / sequence coverage/ sequence score value\n",
    "\n",
    "uniprot_domain_wt_wtemplates['t_date_modified'] = datetime.datetime.now()\n",
    "uniprot_domain_mut_wtemplates['t_date_modified'] = datetime.datetime.now()\n",
    "\n",
    "unique_id_columns = ['uniprot_id', 'alignment_def', 'max_seq_identity']\n",
    "false_duplicate_columns = ['alignment_identity', 'alignment_coverage', 'alignment_score']\n",
    "\n",
    "def remove_false_duplicates(df):\n",
    "    df_base = df[[c for c in df.columns if c not in false_duplicate_columns]].drop_duplicates()\n",
    "    df_duplicates = (\n",
    "        df[unique_id_columns + false_duplicate_columns]\n",
    "        .sort('alignment_score', ascending=False)\n",
    "        .drop_duplicates(unique_id_columns)\n",
    "    )\n",
    "    df_combined = df_base.merge(df_duplicates, on=unique_id_columns)\n",
    "    return df_combined\n",
    "\n",
    "uniprot_domain_wt_wtemplates = remove_false_duplicates(uniprot_domain_wt_wtemplates)\n",
    "uniprot_domain_mut_wtemplates = remove_false_duplicates(uniprot_domain_mut_wtemplates)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "#%% Sanity check of the data\n",
    "\n",
    "# The lengths should be the same because they are just the forward and backward mutations\n",
    "assert len(uniprot_domain_wt_wtemplates) == len(uniprot_domain_mut_wtemplates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "#%% Export data that will be relied on by all other programs...\n",
    "uniprot_domain_wt_mut_wtemplates = pd.concat([uniprot_domain_wt_wtemplates, uniprot_domain_mut_wtemplates], ignore_index=True)\n",
    "\n",
    "uniprot_domain_wt_wtemplates.to_pickle(\n",
    "    constants.protherm_data_path + 'parsed_data{}/uniprot_domain_wt_wtemplates.pickle'.format(version_suffix))\n",
    "uniprot_domain_mut_wtemplates.to_pickle(\n",
    "    constants.protherm_data_path + 'parsed_data{}/uniprot_domain_mut_wtemplates.pickle'.format(version_suffix))\n",
    "uniprot_domain_wt_mut_wtemplates.to_pickle(data_path + 'core/uniprot_domain_wt_mut_wtemplates{}.pickle'.format(version_suffix))\n",
    "\n",
    "# These are the columns that I used as final data previously, but it is wrong!!!\n",
    "# ddG_mean\n",
    "# dTm_mean\n",
    "\n",
    "# These are the columns that should be used by all subsequent functions:\n",
    "# ddg_all_median\n",
    "# dTm_median\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "#%% Save the uniprot_id \\t mutations data to a tsv file to use as input for elaspic\n",
    "unique_uniprot_id_mutations = uniprot_domain_wt_mut_wtemplates[['uniprot_id', 'uniprot_mutation']].drop_duplicates()\n",
    "assert (\n",
    "    unique_uniprot_id_mutations[unique_uniprot_id_mutations['uniprot_id'].str.endswith('_wt')].shape[0] ==\n",
    "    unique_uniprot_id_mutations[~unique_uniprot_id_mutations['uniprot_id'].str.endswith('_wt')].shape[0]\n",
    ")\n",
    "training_tsv = (\n",
    "    unique_uniprot_id_mutations\n",
    "    .groupby(['uniprot_id'])\n",
    "    .agg({'uniprot_mutation': lambda x: ','.join(list(set(x)))})\n",
    "    .reset_index()\n",
    ")\n",
    "training_tsv.to_csv(\n",
    "    '/home/kimlab1/strokach/working/elaspic/input/training_core{}.tsv'.format(version_suffix),\n",
    "    sep='\\t', index=False, header=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "#%% DEBUGGING PURPOSES ONLY!!!\n",
    "# Make sure that mutations with very high ddG values are not errors\n",
    "uniprot_domain_mut_wtemplates_strange_ddg = uniprot_domain_mut_wtemplates[\n",
    "        (pd.notnull(uniprot_domain_mut_wtemplates['ddg_all_mean'])) &\n",
    "        ((uniprot_domain_mut_wtemplates['ddg_all_mean'] > 10) |\n",
    "        (uniprot_domain_mut_wtemplates['ddg_all_mean'] < -10))\n",
    "    ]\n",
    "print(uniprot_domain_mut_wtemplates_strange_ddg[\n",
    "        ['protein_name', 'uniprot_id', 'uniprot_mutation', 'ddg_all_median']\n",
    "    ].drop_duplicates().to_csv(sep='\\t'))\n",
    "\n",
    "\n",
    "# Make sure that we don't have fewer mutations than we did previously\n",
    "uniprot_domain_wt_wtemplates_old = pd.read_pickle(\n",
    "    constants.protherm_data_path + 'parsed_data{}/uniprot_domain_wt_wtemplates.pickle'.format('_v8'))\n",
    "\n",
    "uniprot_domain_wt_wtemplates_new_upmut_set = set(uniprot_domain_wt_wtemplates[['uniprot_id', 'uniprot_mutation']].apply(tuple, axis=1))\n",
    "uniprot_domain_wt_wtemplates_old_upmut_set = set(uniprot_domain_wt_wtemplates_old[['uniprot_id', 'uniprot_mutation']].apply(tuple, axis=1))\n",
    "\n",
    "uniprot_domain_wt_wtemplates_new_extra = uniprot_domain_wt_wtemplates[\n",
    "        ~(uniprot_domain_wt_wtemplates[['uniprot_id', 'uniprot_mutation']]\n",
    "            .apply(tuple, axis=1).isin(uniprot_domain_wt_wtemplates_old_upmut_set))\n",
    "    ]\n",
    "assert len(uniprot_domain_wt_wtemplates_new_extra) == 0\n",
    "\n",
    "uniprot_domain_wt_wtemplates_old_extra = uniprot_domain_wt_wtemplates_old[\n",
    "        ~(uniprot_domain_wt_wtemplates_old[['uniprot_id', 'uniprot_mutation']]\n",
    "            .apply(tuple, axis=1).isin(uniprot_domain_wt_wtemplates_new_upmut_set))\n",
    "    ]\n",
    "assert len(uniprot_domain_wt_wtemplates_old_extra) == 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "#%% PART 5 - UPLOAD DATA TO THE DATABASE\n",
    "###################################################################################################\n",
    "\n",
    "#%%\n",
    "uniprot_domain_wt_mut_wtemplates = pd.read_pickle(\n",
    "    data_path + 'core/uniprot_domain_wt_mut_wtemplates{}.pickle'.format(version_suffix))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "#%% Divide the uniprot domain mutation data into uniprot_domain table and uniprot_domain_template table\n",
    "key_columns = ['uniprot_id', 'alignment_def', 'max_seq_identity']\n",
    "\n",
    "uniprot_domain_columns = [\n",
    "    'uniprot_id', 'pdbfam_name', 'pdbfam_idx', 'pfam_clan',\n",
    "    'alignment_def', 'pfam_names', 'alignment_subdefs', 'path_to_data', 'max_seq_identity']\n",
    "\n",
    "uniprot_domain_template_columns = [\n",
    "    'template_errors', 'cath_id', 'domain_start', 'domain_end',\n",
    "    'domain_def', 'alignment_identity', 'alignment_coverage', 'alignment_score',\n",
    "    't_date_modified']\n",
    "\n",
    "uniprot_domain_wt_mut_final = (\n",
    "    uniprot_domain_wt_mut_wtemplates\n",
    "    .drop_duplicates(subset=uniprot_domain_columns + uniprot_domain_template_columns)\n",
    "    .dropna(subset=['domain_def'])\n",
    ")\n",
    "\n",
    "assert uniprot_domain_wt_mut_final.shape[0] == uniprot_domain_wt_mut_wtemplates.drop_duplicates(key_columns).shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "#%%\n",
    "engine = sa.create_engine('mysql://elaspic:elaspic@192.168.6.19/elaspic_training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "#%% Fixing my fuckups!! Coment out when doing things for real\n",
    "\n",
    "# Back up data, because you will truncate the entire table\n",
    "\n",
    "path_to_temp_backup = '/home/kimlab1/strokach/tmp/'\n",
    "\n",
    "uniprot_domain = pd.read_sql_table('uniprot_domain', engine)\n",
    "uniprot_domain_template = pd.read_sql_table('uniprot_domain_template', engine)\n",
    "uniprot_domain_model = pd.read_sql_table('uniprot_domain_model', engine)\n",
    "uniprot_domain_mutation = pd.read_sql_table('uniprot_domain_mutation', engine)\n",
    "\n",
    "uniprot_domain.to_pickle(path_to_temp_backup + 'uniprot_domain.pickle')\n",
    "uniprot_domain_template.to_pickle(path_to_temp_backup + 'uniprot_domain.pickle')\n",
    "uniprot_domain_model.to_pickle(path_to_temp_backup + 'uniprot_domain.pickle')\n",
    "uniprot_domain_mutation.to_pickle(path_to_temp_backup + 'uniprot_domain.pickle')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "#%% Save the uniprot_domain and uniprot_domain_template tables to the sql database\n",
    "max_uniprot_domain_id = pd.read_sql_query('select max(uniprot_domain_id) from uniprot_domain', engine).values[0,0]\n",
    "if max_uniprot_domain_id is None:\n",
    "    max_uniprot_domain_id = 0\n",
    "\n",
    "uniprot_domain_wt_mut_final['uniprot_domain_id'] = range(\n",
    "    max_uniprot_domain_id + 1,\n",
    "    max_uniprot_domain_id + 1 + len(uniprot_domain_wt_mut_final)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "#%%\n",
    "sql_query = \"\"\"\n",
    "select uniprot_id, alignment_def, max_seq_identity, cath_id\n",
    "from uniprot_domain\n",
    "join uniprot_domain_template using (uniprot_domain_id)\n",
    "\"\"\"\n",
    "precalculated_data = pd.read_sql_query(sql_query, engine)\n",
    "precalculated_data['max_seq_identity'] = precalculated_data['max_seq_identity'].astype(int)\n",
    "precalculated_data_tuples = set(precalculated_data.apply(tuple, axis=1))\n",
    "\n",
    "uniprot_domain_wt_mut_final_new = uniprot_domain_wt_mut_final[\n",
    "        ~(uniprot_domain_wt_mut_final[['uniprot_id', 'alignment_def', 'max_seq_identity', 'cath_id']]\n",
    "            .apply(tuple, axis=1).isin(precalculated_data_tuples))\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "#%%\n",
    "uniprot_domain_wt_mut_final_new[['uniprot_domain_id'] + uniprot_domain_columns].to_sql(\n",
    "    'uniprot_domain', engine, if_exists='append', index=False)\n",
    "\n",
    "uniprot_domain_wt_mut_final_new[['uniprot_domain_id'] + uniprot_domain_template_columns].to_sql(\n",
    "    'uniprot_domain_template', engine, if_exists='append', index=False)\n",
    "\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
